{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pdfplumber\n",
    "import re\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "import csv\n",
    "\n",
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "import faiss\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\vyshn\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\vyshn\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Download NLTK data (you only need to run this once)\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a function to clean and preprocess text\n",
    "def clean_text(text):\n",
    "    \"\"\"\n",
    "    Cleans and preprocesses the text for vectorization.\n",
    "    - Removes headers, footers, special characters, and unnecessary whitespace.\n",
    "    - Tokenizes text into sentences.\n",
    "    - Removes stop words.\n",
    "    \"\"\"\n",
    "    # Remove headers and footers (assumption: they repeat every page)\n",
    "    text = re.sub(r'CareerSeva.Com\\'s Destiny Designers: A Comprehensive Guide to Career Counseling Entrepreneurship in India - 2023*|CareerSeva.Com -A2Z Career Guidance & Planning    © Sfurti Media Production, Pune.2023*', '', text)\n",
    "\n",
    "    # Remove special characters and excessive whitespace\n",
    "    text = re.sub(r'\\s+', ' ', text)  # Replace multiple spaces with a single space\n",
    "    text = re.sub(r'[^\\w\\s]', '', text)  # Remove special characters\n",
    "    \n",
    "    # Tokenize text into sentences\n",
    "    sentences = nltk.sent_tokenize(text)\n",
    "    \n",
    "    # Remove stop words\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    processed_sentences = []\n",
    "    for sentence in sentences:\n",
    "        words = nltk.word_tokenize(sentence)\n",
    "        filtered_words = [word for word in words if word.lower() not in stop_words]\n",
    "        processed_sentences.append(' '.join(filtered_words))\n",
    "    \n",
    "    # Return cleaned sentences as a single string\n",
    "    return ' '.join(processed_sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a function to extract text from a PDF\n",
    "def extract_text_from_pdf(pdf_path):\n",
    "    \"\"\"\n",
    "    Extracts text from a PDF file while handling images, tables, bullet points, headers, and footers.\n",
    "    \"\"\"\n",
    "    extracted_text = \"\"\n",
    "    \n",
    "    # Open the PDF file\n",
    "    with pdfplumber.open(pdf_path) as pdf:\n",
    "        # Iterate through each page\n",
    "        for page in pdf.pages:\n",
    "            # Extract text from the page\n",
    "            page_text = page.extract_text()\n",
    "            \n",
    "            # Skip pages with no text\n",
    "            if not page_text:\n",
    "                continue\n",
    "            \n",
    "            # Clean the extracted text\n",
    "            cleaned_text = clean_text(page_text)\n",
    "            \n",
    "            # Append cleaned text\n",
    "            extracted_text += cleaned_text + \" \"\n",
    "    \n",
    "    return extracted_text.strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_footer(text):\n",
    "    pattern = r\"CareerSevaCom A2Z Career Guidance Planning Sfurti Media Production Pune2023 Page \\d+\"\n",
    "    cleaned_text = re.sub(pattern, '', text)\n",
    "    return cleaned_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_numeric_tokens(tokens_string):\n",
    "    tokens = tokens_string.split()\n",
    "    filtered_tokens = [token for token in tokens if not token.isdigit()]\n",
    "    result_string = ' '.join(filtered_tokens)\n",
    "    return result_string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_tokens_between(tokens_string):\n",
    "    \"\"\"\n",
    "    Remove page containing table of contents\n",
    "    \"\"\"\n",
    "    # Split the space-separated string into a list of tokens\n",
    "    tokens = tokens_string.split()\n",
    "    \n",
    "    # Initialize flags for tracking token range\n",
    "    within_range = False\n",
    "    filtered_tokens = []\n",
    "    count = 0\n",
    "    \n",
    "    for token in tokens:\n",
    "        if token == \"Table\":\n",
    "            within_range = True\n",
    "            count += 1\n",
    "        \n",
    "        if not within_range:\n",
    "            filtered_tokens.append(token)\n",
    "        \n",
    "        if token == \"153\":\n",
    "            within_range = False\n",
    "            count -= 1\n",
    "    \n",
    "    # Join the filtered tokens back into a space-separated string\n",
    "    cleaned_tokens_string = ' '.join(filtered_tokens)\n",
    "    \n",
    "    #print(count)\n",
    "    return cleaned_tokens_string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CareerSevaComs EBook Edition Destiny Designers Comprehensive Guide Career Counseling Entrepreneurship India Teacher Working SelfEmployed Professional Digital Content Creator Housewife Simply Graduate Want Make Rewarding Career Career Counseling Begin Journey Unlocking Secrets Successful Career Counseling Future Educators Coaches Author Editor CareerSevacoms Expert Career Counselors Content Creation Team EBook MRP INR Published Sfurti Media Production Pune Preface fluid tapestry life dreams inter\n"
     ]
    }
   ],
   "source": [
    "# Path to your PDF file\n",
    "pdf_path = 'Dataset.pdf'\n",
    "\n",
    "# Extract and preprocess text from the PDF\n",
    "processed_text = extract_text_from_pdf(pdf_path)\n",
    "\n",
    "# Remove Footer\n",
    "processed_text = remove_footer(processed_text)\n",
    "\n",
    "# Remove Table of Contents page\n",
    "processed_text = remove_tokens_between(processed_text)\n",
    "\n",
    "# Remove number tokens\n",
    "processed_text = remove_numeric_tokens(processed_text)\n",
    "\n",
    "# Print a preview of the processed text\n",
    "print(processed_text[:500])  # Display the first 500 characters to verify"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_tokens_to_csv(tokens_string, filename):\n",
    "    tokens = tokens_string.split()\n",
    "    with open(filename, mode='w', newline='') as file:\n",
    "        writer = csv.writer(file)\n",
    "        writer.writerow(tokens)\n",
    "\n",
    "filename = \"tokens.csv\"\n",
    "save_tokens_to_csv(processed_text, filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batches: 100%|██████████| 1/1 [00:03<00:00,  3.53s/it]\n"
     ]
    }
   ],
   "source": [
    "# Load the model\n",
    "model = SentenceTransformer('multi-qa-mpnet-base-dot-v1') # Chosen since it is recommended for QnA systems and trained on semantic results\n",
    "\n",
    "sentences = nltk.sent_tokenize(processed_text)  # Tokenize the text into sentences\n",
    "\n",
    "# Vectorize the sentences using the selected model\n",
    "sentence_embeddings = model.encode(sentences, show_progress_bar=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated Embeddings for QA Task:\n",
      "[[ 1.17667601e-01  2.72827625e-01 -3.90777200e-01 -2.75067508e-01\n",
      "   2.74234056e-01 -7.43905678e-02  3.50089252e-01  2.12362513e-01\n",
      "   4.69452925e-02  6.91179410e-02  1.38827920e-01  3.93895924e-01\n",
      "   1.55625353e-02  2.12807357e-01  8.15760251e-03  4.95283306e-03\n",
      "   5.16690165e-02  1.80346072e-01  5.47197461e-03 -4.97416928e-02\n",
      "  -2.28070356e-02  7.05097988e-02  1.78358331e-02  3.27904463e-01\n",
      "  -5.88320792e-02 -2.87404656e-01  2.69400895e-01  2.50371575e-01\n",
      "  -4.97082844e-02 -1.37031823e-01 -1.36470608e-02  5.36916144e-02\n",
      "  -1.90986499e-01 -5.61979413e-03 -9.72293274e-05 -6.03161156e-02\n",
      "   5.71936928e-02 -1.60974592e-01 -3.33396614e-01 -5.22207618e-02\n",
      "  -6.39481992e-02  1.17418274e-01 -2.85843730e-01 -1.46881565e-01\n",
      "  -3.08612734e-02  3.47103477e-02  7.96274096e-03  1.37949735e-02\n",
      "   2.04369992e-01  4.76509742e-02  3.84025306e-01 -2.79168993e-01\n",
      "   1.70563802e-01 -1.59542546e-01 -1.09696470e-01  2.17482358e-01\n",
      "  -1.36325270e-01 -3.32927108e-02  5.45635581e-01 -4.67745066e-02\n",
      "  -3.27095032e-01  1.60729960e-01 -1.47907242e-01 -1.34799406e-01\n",
      "   5.20347320e-02 -2.16126904e-01  4.30967331e-01 -3.55029345e-01\n",
      "   3.77121121e-01  1.62116252e-02  4.39740866e-01 -1.36417001e-01\n",
      "  -2.22201079e-01 -1.79472670e-01  2.52369851e-01  8.00502300e-02\n",
      "  -1.80780068e-02  3.94686133e-01 -2.73122549e-01  1.94250852e-01\n",
      "   2.89332122e-01  2.10640773e-01 -2.97176123e-01 -6.84433728e-02\n",
      "   3.90300632e-01  7.88987800e-02 -2.69486099e-01 -3.02599639e-01\n",
      "  -3.27063389e-02 -7.29031116e-02  7.85782114e-02  8.88541061e-03\n",
      "   7.34414756e-02  5.22401668e-02  1.85966715e-01 -2.56716490e-01\n",
      "   2.43323743e-01  2.25982815e-01 -3.37461792e-02  3.57166946e-01\n",
      "   1.63181499e-01  1.64723098e-01  2.22245529e-01 -7.50643834e-02\n",
      "  -3.37291397e-02 -3.19956928e-01  3.66126120e-01 -1.88063011e-02\n",
      "  -6.70625418e-02  4.63916004e-01  2.19736159e-01  2.87853539e-01\n",
      "   4.37777452e-02 -1.96690485e-02 -6.18780494e-01  4.45869192e-02\n",
      "  -3.45900297e-01 -1.47280082e-01 -3.36718142e-01  2.77863622e-01\n",
      "  -4.61591221e-02  7.72901550e-02 -1.95345253e-01  1.20847955e-01\n",
      "   1.49594828e-01 -6.90005496e-02 -2.99978256e-03 -1.89388916e-02\n",
      "   5.43084815e-02 -2.61604525e-02 -2.09231615e-01  2.38412008e-01\n",
      "  -5.66258840e-02  3.01443543e-02  1.53292164e-01  1.63236484e-01\n",
      "   1.71535060e-01  3.35977040e-02 -1.47043496e-01 -1.32237747e-01\n",
      "   1.67266205e-01 -6.81372285e-02  1.07088871e-01 -4.38235067e-02\n",
      "  -1.97673306e-01 -1.29834831e-01 -1.98531374e-02 -9.34362411e-02\n",
      "  -2.33773589e-02 -1.97188184e-02 -9.83767733e-02 -6.77178204e-02\n",
      "   1.99216418e-02  3.42886120e-01  3.50197732e-01  5.49923070e-02\n",
      "   4.11810130e-01  2.55978107e-01 -1.51905298e-01  2.29990423e-01\n",
      "   3.59730273e-01  3.83233249e-01 -3.20272893e-02  6.91644698e-02\n",
      "  -2.34861121e-01  1.68633357e-01  1.31549180e-01 -9.22437832e-02\n",
      "  -9.52249616e-02 -5.55866249e-02 -1.37394398e-01 -1.01947218e-01\n",
      "  -1.64314151e-01  6.18069410e-01  2.87315827e-02 -3.87692079e-02\n",
      "   3.97201151e-01 -1.44218668e-01 -3.12871695e-01  3.90818089e-01\n",
      "  -2.61123627e-01 -3.83044749e-01  2.03465730e-01 -1.47360772e-01\n",
      "   2.69477934e-01 -3.58360469e-01 -3.67733613e-02  2.79850066e-01\n",
      "  -1.54262424e-01  2.60949899e-02 -2.77822584e-01 -6.67191744e-02\n",
      "  -5.60607314e-01  3.03958245e-02 -3.67177762e-02 -1.01875834e-01\n",
      "   2.42454335e-02 -4.42969315e-02  3.55122507e-01 -3.72416914e-01\n",
      "   2.57504255e-01  3.64921629e-01 -1.73596591e-01  5.62828816e-02\n",
      "  -8.14351812e-02 -1.00793522e-02  1.23360924e-01 -1.98243529e-01\n",
      "  -1.22621372e-01  2.25944251e-01  5.62352687e-02  3.05308178e-02\n",
      "  -1.47152603e-01 -7.20350444e-02 -1.44868121e-01 -1.68369055e-01\n",
      "   3.68984938e-01 -3.68314236e-02  8.25197697e-02  2.38743469e-01\n",
      "   3.45564447e-02  1.68343157e-01 -5.57118535e-01  9.83717963e-02\n",
      "  -7.84212649e-02 -1.03233196e-02 -6.55651763e-02  2.08141878e-02\n",
      "  -1.86928511e-01  4.56139088e-01 -2.11224139e-01  1.31408513e-01\n",
      "   5.46724275e-02 -2.04666734e-01 -4.98067468e-01  3.26505125e-01\n",
      "   4.39645708e-01  2.26609528e-01 -2.20142946e-01 -4.76669848e-01\n",
      "   2.00631306e-01 -6.12738766e-02  1.43813848e-01  1.12792365e-01\n",
      "  -9.80774388e-02  5.18179953e-01  5.89108616e-02  1.08770937e-01\n",
      "   3.03020030e-02 -1.54216923e-02 -8.36738870e-02 -2.14654893e-01\n",
      "  -2.23967105e-01 -1.07695878e-01  2.53908277e-01  1.43049002e-01\n",
      "  -2.96512917e-02 -1.41530156e-01  2.17715055e-02  2.58853436e-01\n",
      "   5.14191911e-02 -9.14611071e-02  2.12655783e-01  2.37381458e-03\n",
      "  -5.46446964e-02 -1.42921820e-01  1.25588506e-01  1.28143862e-01\n",
      "   7.38129616e-02  2.21631899e-01  1.82075813e-01 -2.20595658e-01\n",
      "   9.01122466e-02 -3.36901098e-01  3.32948029e-01  1.41256392e-01\n",
      "  -1.46896318e-01  4.52594668e-01 -1.87374605e-03  1.66985393e-01\n",
      "  -1.91060185e-01 -1.50692537e-01 -1.86787367e-01 -1.92264929e-01\n",
      "   3.98502126e-03  1.15120076e-01  3.34177166e-02  4.31340411e-02\n",
      "   1.56653419e-01 -7.39986971e-02  3.49678189e-01 -1.01483271e-01\n",
      "  -2.55655572e-02 -3.01775597e-02 -2.63084114e-01 -1.76283158e-02\n",
      "  -5.87195233e-02  2.42177144e-01 -1.40505075e-01  2.13410586e-01\n",
      "  -2.40906253e-01 -3.21285725e-01  1.36609375e-01  3.54445100e-01\n",
      "  -5.71424216e-02  8.05649720e-03  4.31449324e-01 -5.08548617e-02\n",
      "  -1.17030315e-01 -3.34684402e-01 -3.13077122e-01  5.58232330e-02\n",
      "   9.87824053e-04 -1.62767500e-01 -3.94154191e-02 -9.78084654e-02\n",
      "  -1.11644343e-01  7.28801638e-02 -1.26679674e-01  6.95321858e-02\n",
      "  -1.60125211e-01  6.27252236e-02 -2.33575925e-01  6.58106506e-02\n",
      "  -1.02808282e-01 -2.64664918e-01 -1.38429478e-01 -3.19141805e-01\n",
      "   1.21707208e-01 -6.98096603e-02  9.96644497e-02  1.80524722e-01\n",
      "  -8.36583599e-02  3.26766558e-02 -3.02314516e-02  2.80097395e-01\n",
      "  -2.33594447e-01 -8.69199932e-02  1.44547001e-01 -2.47293726e-01\n",
      "  -1.33833617e-01 -4.66794893e-03 -5.32038391e-01  2.59342581e-01\n",
      "  -1.34408489e-01 -9.39879492e-02  1.89512640e-01  4.46444929e-01\n",
      "  -4.09460962e-02 -7.72626325e-02 -1.98274609e-02  2.60538131e-01\n",
      "  -4.77223918e-02 -2.78634906e-01  1.59716681e-02  9.33773816e-02\n",
      "   1.23456448e-01 -2.47945964e-01  3.26343626e-01 -5.79519644e-02\n",
      "   6.83126450e-02  1.87277555e-01  1.54666796e-01 -2.85858065e-01\n",
      "   1.91645503e-01  1.19826682e-02  3.27199012e-01 -3.94151174e-02\n",
      "   1.15889773e-01 -1.51356906e-01  1.92052782e-01  3.25436890e-01\n",
      "   3.96220237e-02  1.29669607e-01  1.11985646e-01 -9.23966989e-04\n",
      "  -1.01458624e-01  9.05821323e-02 -3.49541754e-01 -6.82440400e-02\n",
      "   2.91762799e-01  1.36312395e-01  5.00051677e-01 -1.83310896e-01\n",
      "   2.12109312e-02  1.58444606e-03  8.94436426e-03  7.28478432e-02\n",
      "   1.36791497e-01  1.47351056e-01 -2.93528438e-01  2.63516396e-01\n",
      "   1.61182284e-01 -1.35941565e-01 -9.12050605e-02  1.77148089e-01\n",
      "  -4.76169735e-01 -2.17040747e-01 -4.25498992e-01  1.23568669e-01\n",
      "  -5.47050163e-02 -2.20860407e-01 -4.95849028e-02  9.55220014e-02\n",
      "   9.23327804e-02 -2.42832564e-02  1.54155884e-02  4.48639132e-02\n",
      "  -1.93410814e-01  4.26262245e-02 -3.09236467e-01  6.11824356e-02\n",
      "  -1.58881515e-01 -1.20195281e-02  1.66502878e-01 -4.92799997e-01\n",
      "  -1.97659247e-02  1.61132604e-01  2.70415902e-01 -3.55645359e-01\n",
      "  -1.61536068e-01  9.04185399e-02 -1.97714567e-01 -6.72247931e-02\n",
      "  -1.83170244e-01 -7.16074482e-02  2.49013707e-01  2.37631589e-01\n",
      "   2.07329273e-01 -1.46422774e-01 -2.95362681e-01  4.57609519e-02\n",
      "   1.90341920e-02 -1.70557424e-01  1.15229987e-01  1.08055547e-01\n",
      "   3.38436030e-02 -7.55231678e-02  1.38385952e-01  6.70196712e-02\n",
      "  -3.30769330e-01  1.47423610e-01  6.45365715e-01  1.28309697e-01\n",
      "   2.08706483e-02 -1.82871208e-01  1.15719348e-01 -2.26131007e-02\n",
      "  -2.76820987e-01  9.03990120e-02 -1.33767910e-02  7.89484233e-02\n",
      "  -6.23536259e-02  2.94691443e-01 -2.06690326e-01 -2.04292953e-01\n",
      "  -1.98156208e-01  1.86003372e-02 -2.65202727e-02 -6.55186474e-02\n",
      "   3.19433898e-01  1.02709055e+00  5.07938743e-01  1.02240115e-01\n",
      "   1.60528332e-01 -3.63155603e-02  1.00176714e-01  3.19653973e-02\n",
      "   1.95003286e-01 -1.78815678e-01  2.09017038e-01 -1.10380918e-01\n",
      "  -3.17107812e-02  5.78648560e-02  4.13296342e-01  6.99116886e-02\n",
      "  -2.82741264e-02 -1.02149695e-03 -2.78241754e-01  1.31473511e-01\n",
      "  -8.97268653e-02 -2.89815933e-01 -1.90221429e-01  5.90672977e-02\n",
      "   3.71746093e-01  7.17906877e-02  7.58440644e-02  4.64618951e-02\n",
      "  -2.13074058e-01 -4.07372087e-01  5.46356365e-02 -9.70801413e-02\n",
      "   8.47942233e-02 -9.49755087e-02  8.28130618e-02  1.10959291e-01\n",
      "  -5.36741138e-01  3.32214266e-01 -3.74736674e-02  2.46901169e-01\n",
      "  -3.72168839e-01 -8.30719247e-02 -2.60060281e-03  8.84716585e-03\n",
      "  -9.05594826e-02  4.72501218e-02  2.28604808e-01  4.38993216e-01\n",
      "  -1.92980200e-01  2.13058621e-01  1.56586602e-01 -1.31569169e-02\n",
      "   2.41281912e-02 -6.49506152e-01  5.19816279e-02  2.28763953e-01\n",
      "  -1.01928428e-01  1.03664935e-01  3.81729215e-01 -1.92684010e-01\n",
      "   9.14442912e-03  3.19271147e-01  4.83635888e-02  3.37808505e-02\n",
      "  -1.21522166e-01  1.02423973e-01  2.29098126e-02 -9.61928219e-02\n",
      "   1.02658436e-01 -6.91384524e-02  1.15721032e-01  2.47975718e-02\n",
      "  -7.30738938e-02 -3.00320745e-01 -4.71093148e-01 -2.77526736e-01\n",
      "  -1.09358847e-01 -1.62532851e-01  1.65976748e-01  8.84049162e-02\n",
      "  -4.45371449e-01 -1.64781228e-01  1.68623596e-01  1.21169917e-01\n",
      "  -8.63888636e-02 -1.56745464e-01  7.65587091e-02 -7.59514868e-02\n",
      "   2.92054355e-01 -2.09740289e-02 -7.33934790e-02 -5.12166500e-01\n",
      "   7.58878142e-02  4.28172722e-02 -2.22285211e-01 -5.07253408e-01\n",
      "   9.89951044e-02  2.82758266e-01  2.05171853e-02  3.45669091e-01\n",
      "  -3.13972950e-01 -2.13993192e-02  1.99022010e-01  3.24954361e-01\n",
      "   8.91526043e-02 -2.14575320e-01 -4.32389319e-01 -4.87949774e-02\n",
      "   8.45708400e-02 -2.78013684e-02  1.92088544e-01  8.93508345e-02\n",
      "   1.09534279e-01 -7.88413584e-02  6.22681603e-02 -1.00244984e-01\n",
      "  -3.04952323e-01  2.64128208e-01  3.08688402e-01 -4.01325524e-01\n",
      "   4.18101370e-01  1.55790746e-01 -2.44486779e-02 -2.09300965e-01\n",
      "   5.32619655e-01 -3.14262033e-01 -1.01940453e-01  1.14189379e-01\n",
      "  -1.57224655e-01 -2.91336119e-01  3.20249379e-01  1.65092945e-01\n",
      "  -3.84561196e-02  3.99961293e-01 -1.08926296e-02  7.13996589e-02\n",
      "   2.11815730e-01 -9.32008550e-02  1.20882101e-01 -1.53464496e-01\n",
      "  -6.56289458e-02 -1.18864872e-01  5.12557209e-01  1.92919105e-01\n",
      "   1.79532170e-01 -2.02485085e-01 -3.41410488e-01 -1.45607777e-02\n",
      "   1.38189852e-01  1.48008302e-01 -3.54721189e-01  6.72931969e-03\n",
      "   1.45617694e-01  1.76573634e-01 -2.32288018e-01 -1.55571759e-01\n",
      "   7.07895160e-02  2.85325706e-01  1.98169157e-01  1.37888387e-01\n",
      "  -1.22967705e-01  2.56876409e-01 -2.39240959e-01  2.86661863e-01\n",
      "   1.77604526e-01  1.84853580e-02  2.37595335e-01  9.15594399e-02\n",
      "  -2.20013723e-01  3.86993438e-01  7.97115192e-02 -1.71408236e-01\n",
      "   2.95857728e-01  8.50828215e-02  3.06179166e-01 -1.20573305e-01\n",
      "   7.61564374e-02 -2.81982541e-01 -5.29278256e-02 -3.56040925e-01\n",
      "   1.53855145e-01 -4.95099276e-02 -1.74772352e-01 -7.36253336e-02\n",
      "  -2.29042158e-01 -6.36560917e-02  1.04386687e-01  3.76743793e-01\n",
      "  -2.28136316e-01 -1.72627971e-01  4.62533347e-02  2.38479227e-02\n",
      "   1.05161503e-01  4.40864176e-01 -1.33947775e-01 -1.45941079e-01\n",
      "  -3.27493906e-01 -8.49062651e-02 -3.19084913e-01  2.71981597e-01\n",
      "   2.20846310e-01 -2.88651288e-01 -2.17452139e-01 -1.01425067e-01\n",
      "  -2.68977821e-01 -3.12357992e-01 -5.97037841e-03  1.50145531e-01\n",
      "   5.99222183e-02  1.57112718e-01 -1.69929847e-01  3.30579579e-01\n",
      "  -3.25316638e-01 -9.52859819e-02  3.35347265e-01  2.11714402e-01\n",
      "   1.58383593e-01 -1.26312882e-01  1.44617870e-01 -5.11864945e-02\n",
      "  -1.11907586e-01 -2.57768631e-02 -3.50177467e-01 -2.02312395e-01\n",
      "   6.71931133e-02 -1.09549224e-01 -9.35641080e-02  8.39742348e-02\n",
      "   1.70286670e-01  8.62544589e-03  3.17720138e-02  7.65377954e-02\n",
      "  -1.24626882e-01  1.48083284e-01 -1.74681813e-01  2.29550183e-01\n",
      "   1.31658807e-01  2.51184165e-01 -1.09369650e-01 -3.59242797e-01\n",
      "   1.42222583e-01  9.44032297e-02  9.67862308e-02 -4.27817553e-02\n",
      "  -5.11751831e-01 -3.61327939e-02  2.32838988e-01 -1.06167078e-01\n",
      "  -5.94031252e-02  3.59394372e-01  1.17610469e-02  5.71227819e-02\n",
      "  -1.52086183e-01 -1.90480009e-01  3.17039490e-01  3.05931270e-01\n",
      "  -1.00212686e-01 -2.61554644e-02  1.07832804e-01 -6.57656714e-02\n",
      "   2.61358559e-01 -1.30979389e-01  1.50485963e-01 -2.70602196e-01\n",
      "  -3.65361750e-01  2.61361629e-01  2.06152648e-01  1.07496105e-01\n",
      "   3.60894561e-01  1.62729174e-01  3.05009782e-01  7.52556920e-02\n",
      "  -1.21338382e-01  1.49815470e-01 -3.55687179e-02  7.92841241e-02\n",
      "  -4.79843915e-02 -6.97216034e-01  3.83801535e-02 -3.08094714e-02\n",
      "   1.21822938e-01 -1.86375603e-01  1.69661015e-01  1.26450844e-02\n",
      "  -1.73416138e-01 -1.06074177e-01 -3.90397877e-01  1.67194203e-01\n",
      "  -4.60683331e-02 -6.17664829e-02  9.19113085e-02  2.74612963e-01\n",
      "  -7.74992332e-02  9.86790881e-02 -1.04724325e-01  2.04297677e-02\n",
      "   5.01333959e-02 -1.59050614e-01 -7.80237168e-02  1.58266202e-01\n",
      "  -3.65625881e-02 -5.18546999e-03 -1.29676536e-01  7.92056695e-02\n",
      "  -1.02367185e-01 -7.05154017e-02  6.92110062e-02 -2.73937106e-01]]\n"
     ]
    }
   ],
   "source": [
    "print(\"Generated Embeddings for QA Task:\")\n",
    "print(sentence_embeddings[:5])  # Print the first 5 embeddings to verify"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Embeddings saved in FAISS index.\n"
     ]
    }
   ],
   "source": [
    "# Convert the embeddings to a numpy array (required by FAISS)\n",
    "embeddings_array = np.array(sentence_embeddings).astype('float32')\n",
    "\n",
    "# Initialize the FAISS index\n",
    "embedding_dimension = embeddings_array.shape[1]  # Dimensions of the embeddings (should be 768 for mpnet-base)\n",
    "index = faiss.IndexFlatL2(embedding_dimension)  # L2 distance is used for similarity search\n",
    "\n",
    "# Add embeddings to the FAISS index\n",
    "index.add(embeddings_array)\n",
    "\n",
    "# Save the FAISS index for future use\n",
    "faiss.write_index(index, \"faiss_index.bin\")\n",
    "\n",
    "# Also, save sentences for retrieving the context later\n",
    "import pickle\n",
    "\n",
    "with open('sentences.pkl', 'wb') as f:\n",
    "    pickle.dump(sentences, f)\n",
    "\n",
    "print(\"Embeddings saved in FAISS index.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def chunk_text(text, max_length=400):\n",
    "    \"\"\"\n",
    "    Chunks the text into smaller parts based on a maximum length.\n",
    "    \"\"\"\n",
    "    # Tokenize the text into sentences\n",
    "    sentences = nltk.sent_tokenize(text)\n",
    "    \n",
    "    chunks = []\n",
    "    current_chunk = []\n",
    "    current_length = 0  # Reset current length for every new chunk\n",
    "\n",
    "    # Create chunks by grouping sentences\n",
    "    for sentence in sentences:\n",
    "        # Clean up the sentence to avoid special character issues\n",
    "        sentence = sentence.strip()\n",
    "\n",
    "        # Split sentence into words\n",
    "        sentence_length = len(sentence.split())\n",
    "\n",
    "        if current_length + sentence_length > max_length:\n",
    "            # If adding the current sentence exceeds max_length, finalize the current chunk\n",
    "            if current_chunk:  # Ensure not adding empty chunks\n",
    "                chunks.append(\" \".join(current_chunk))\n",
    "            current_chunk = [sentence]  # Start a new chunk with the current sentence\n",
    "            current_length = sentence_length  # Reset current length\n",
    "        else:\n",
    "            # Add sentence to the current chunk\n",
    "            current_chunk.append(sentence)\n",
    "            current_length += sentence_length  # Update the current length\n",
    "    \n",
    "    # Add the last chunk if it has any sentences\n",
    "    if current_chunk:\n",
    "        chunks.append(\" \".join(current_chunk))\n",
    "    \n",
    "    # Debugging: Print out the chunks to verify\n",
    "    print(f\"Generated {len(chunks)} chunks:\")\n",
    "    for i, chunk in enumerate(chunks[:5]):  # Show only the first 5 chunks for brevity\n",
    "        print(f\"Chunk {i+1} (Length: {len(chunk.split())} words): {chunk[:200]}...\")  # Print the first 200 characters of each chunk\n",
    "    \n",
    "    return chunks\n",
    "\n",
    "\n",
    "\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "def score_chunks(query, chunks, model):\n",
    "    \"\"\"\n",
    "    Scores each chunk based on its relevance to the query using cosine similarity.\n",
    "    \"\"\"\n",
    "    query_embedding = model.encode([query], show_progress_bar=False)\n",
    "    chunk_embeddings = model.encode(chunks, show_progress_bar=True)\n",
    "\n",
    "    # Compute cosine similarity scores\n",
    "    scores = cosine_similarity(query_embedding, chunk_embeddings).flatten()\n",
    "    \n",
    "    # Pair chunks with their scores\n",
    "    chunk_scores = list(zip(chunks, scores))\n",
    "    \n",
    "    # Sort by scores in descending order\n",
    "    chunk_scores = sorted(chunk_scores, key=lambda x: x[1], reverse=True)\n",
    "    \n",
    "    return chunk_scores\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nquery = \"What are the best career options in technology?\"\\nrelevant_sections = retrieve_relevant_sections(query, model, index, sentences, top_n=3)\\nprint(\"Top relevant sections:\", relevant_sections)'"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load the FAISS index\n",
    "index = faiss.read_index(\"faiss_index.bin\")\n",
    "\n",
    "# Load sentences\n",
    "with open('sentences.pkl', 'rb') as f:\n",
    "    sentences = pickle.load(f)\n",
    "\n",
    "# Function to retrieve top N relevant sections\n",
    "def retrieve_relevant_sections(query, model, index, sentences, top_n=5):\n",
    "    \"\"\"\n",
    "    Retrieves the top N relevant sections for a given query using FAISS.\n",
    "    \"\"\"\n",
    "    # Encode the query using the same model\n",
    "    query_embedding = model.encode([query])[0].astype('float32')\n",
    "\n",
    "    # Search in the FAISS index\n",
    "    distances, indices = index.search(np.array([query_embedding]), top_n)\n",
    "\n",
    "    # Retrieve the corresponding sentences/sections\n",
    "    relevant_sections = [sentences[idx] for idx in indices[0]]\n",
    "    \n",
    "    return relevant_sections\n",
    "\n",
    "# Example usage\n",
    "'''\n",
    "query = \"What are the best career options in technology?\"\n",
    "relevant_sections = retrieve_relevant_sections(query, model, index, sentences, top_n=3)\n",
    "print(\"Top relevant sections:\", relevant_sections)'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "# Load the QA model pipeline from HuggingFace\n",
    "qa_pipeline = pipeline(\"question-answering\", model=\"deepset/roberta-base-squad2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Combined text length (in words): 139560\n",
      "Generated 1 chunks:\n",
      "Chunk 1 (Length: 139560 words): CareerSevaComs EBook Edition Destiny Designers Comprehensive Guide Career Counseling Entrepreneurship India Teacher Working SelfEmployed Professional Digital Content Creator Housewife Simply Graduate ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batches: 100%|██████████| 1/1 [00:01<00:00,  1.88s/it]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[31], line 31\u001b[0m\n\u001b[0;32m     29\u001b[0m \u001b[38;5;66;03m# Example usage\u001b[39;00m\n\u001b[0;32m     30\u001b[0m query \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mWhat are the best career options in technology?\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m---> 31\u001b[0m answer, top_chunk \u001b[38;5;241m=\u001b[39m \u001b[43mretrieve_and_answer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mquery\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mindex\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msentences\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mqa_pipeline\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtop_n\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m5\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m     33\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mMost Relevant Chunk:\u001b[39m\u001b[38;5;124m\"\u001b[39m, top_chunk)\n\u001b[0;32m     34\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAnswer:\u001b[39m\u001b[38;5;124m\"\u001b[39m, answer)\n",
      "Cell \u001b[1;32mIn[31], line 22\u001b[0m, in \u001b[0;36mretrieve_and_answer\u001b[1;34m(query, model, index, sentences, qa_pipeline, top_n, max_chunk_length)\u001b[0m\n\u001b[0;32m     19\u001b[0m top_chunk \u001b[38;5;241m=\u001b[39m chunk_scores[\u001b[38;5;241m0\u001b[39m][\u001b[38;5;241m0\u001b[39m]  \u001b[38;5;66;03m# Get the chunk with the highest relevance score\u001b[39;00m\n\u001b[0;32m     21\u001b[0m \u001b[38;5;66;03m# Use the QA model to find the answer in the most relevant chunk\u001b[39;00m\n\u001b[1;32m---> 22\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[43mqa_pipeline\u001b[49m\u001b[43m(\u001b[49m\u001b[43m{\u001b[49m\n\u001b[0;32m     23\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mquestion\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mquery\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     24\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mcontext\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mtop_chunk\u001b[49m\n\u001b[0;32m     25\u001b[0m \u001b[43m\u001b[49m\u001b[43m}\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     27\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m result[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124manswer\u001b[39m\u001b[38;5;124m'\u001b[39m], top_chunk\n",
      "File \u001b[1;32mc:\\Python312\\Lib\\site-packages\\transformers\\pipelines\\question_answering.py:393\u001b[0m, in \u001b[0;36mQuestionAnsweringPipeline.__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m    391\u001b[0m examples \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_args_parser(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    392\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(examples, (\u001b[38;5;28mlist\u001b[39m, \u001b[38;5;28mtuple\u001b[39m)) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(examples) \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[1;32m--> 393\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;21;43m__call__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mexamples\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    394\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__call__\u001b[39m(examples, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Python312\\Lib\\site-packages\\transformers\\pipelines\\base.py:1198\u001b[0m, in \u001b[0;36mPipeline.__call__\u001b[1;34m(self, inputs, num_workers, batch_size, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1196\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39miterate(inputs, preprocess_params, forward_params, postprocess_params)\n\u001b[0;32m   1197\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mframework \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpt\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(\u001b[38;5;28mself\u001b[39m, ChunkPipeline):\n\u001b[1;32m-> 1198\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mnext\u001b[39;49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1199\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43miter\u001b[39;49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1200\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_iterator\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1201\u001b[0m \u001b[43m                \u001b[49m\u001b[43m[\u001b[49m\u001b[43minputs\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_workers\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpreprocess_params\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mforward_params\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpostprocess_params\u001b[49m\n\u001b[0;32m   1202\u001b[0m \u001b[43m            \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1203\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1204\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1205\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m   1206\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrun_single(inputs, preprocess_params, forward_params, postprocess_params)\n",
      "File \u001b[1;32mc:\\Python312\\Lib\\site-packages\\transformers\\pipelines\\pt_utils.py:124\u001b[0m, in \u001b[0;36mPipelineIterator.__next__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    121\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mloader_batch_item()\n\u001b[0;32m    123\u001b[0m \u001b[38;5;66;03m# We're out of items within a batch\u001b[39;00m\n\u001b[1;32m--> 124\u001b[0m item \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mnext\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43miterator\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    125\u001b[0m processed \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39minfer(item, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mparams)\n\u001b[0;32m    126\u001b[0m \u001b[38;5;66;03m# We now have a batch of \"inferred things\".\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Python312\\Lib\\site-packages\\transformers\\pipelines\\pt_utils.py:266\u001b[0m, in \u001b[0;36mPipelinePackIterator.__next__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    263\u001b[0m             \u001b[38;5;28;01mreturn\u001b[39;00m accumulator\n\u001b[0;32m    265\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_last:\n\u001b[1;32m--> 266\u001b[0m     processed \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minfer\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mnext\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43miterator\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mparams\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    267\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mloader_batch_size \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    268\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(processed, torch\u001b[38;5;241m.\u001b[39mTensor):\n",
      "File \u001b[1;32mc:\\Python312\\Lib\\site-packages\\transformers\\pipelines\\base.py:1112\u001b[0m, in \u001b[0;36mPipeline.forward\u001b[1;34m(self, model_inputs, **forward_params)\u001b[0m\n\u001b[0;32m   1110\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m inference_context():\n\u001b[0;32m   1111\u001b[0m         model_inputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_ensure_tensor_on_device(model_inputs, device\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdevice)\n\u001b[1;32m-> 1112\u001b[0m         model_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_forward\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel_inputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mforward_params\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1113\u001b[0m         model_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_ensure_tensor_on_device(model_outputs, device\u001b[38;5;241m=\u001b[39mtorch\u001b[38;5;241m.\u001b[39mdevice(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcpu\u001b[39m\u001b[38;5;124m\"\u001b[39m))\n\u001b[0;32m   1114\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "File \u001b[1;32mc:\\Python312\\Lib\\site-packages\\transformers\\pipelines\\question_answering.py:520\u001b[0m, in \u001b[0;36mQuestionAnsweringPipeline._forward\u001b[1;34m(self, inputs)\u001b[0m\n\u001b[0;32m    518\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124muse_cache\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m inspect\u001b[38;5;241m.\u001b[39msignature(model_forward)\u001b[38;5;241m.\u001b[39mparameters\u001b[38;5;241m.\u001b[39mkeys():\n\u001b[0;32m    519\u001b[0m     model_inputs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124muse_cache\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[1;32m--> 520\u001b[0m output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mmodel_inputs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    521\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(output, \u001b[38;5;28mdict\u001b[39m):\n\u001b[0;32m    522\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m {\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstart\u001b[39m\u001b[38;5;124m\"\u001b[39m: output[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstart_logits\u001b[39m\u001b[38;5;124m\"\u001b[39m], \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mend\u001b[39m\u001b[38;5;124m\"\u001b[39m: output[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mend_logits\u001b[39m\u001b[38;5;124m\"\u001b[39m], \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mexample\u001b[39m\u001b[38;5;124m\"\u001b[39m: example, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39minputs}\n",
      "File \u001b[1;32mc:\\Python312\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Python312\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Python312\\Lib\\site-packages\\transformers\\models\\roberta\\modeling_roberta.py:1500\u001b[0m, in \u001b[0;36mRobertaForQuestionAnswering.forward\u001b[1;34m(self, input_ids, attention_mask, token_type_ids, position_ids, head_mask, inputs_embeds, start_positions, end_positions, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[0;32m   1488\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m   1489\u001b[0m \u001b[38;5;124;03mstart_positions (`torch.LongTensor` of shape `(batch_size,)`, *optional*):\u001b[39;00m\n\u001b[0;32m   1490\u001b[0m \u001b[38;5;124;03m    Labels for position (index) of the start of the labelled span for computing the token classification loss.\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1496\u001b[0m \u001b[38;5;124;03m    are not taken into account for computing the loss.\u001b[39;00m\n\u001b[0;32m   1497\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m   1498\u001b[0m return_dict \u001b[38;5;241m=\u001b[39m return_dict \u001b[38;5;28;01mif\u001b[39;00m return_dict \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39muse_return_dict\n\u001b[1;32m-> 1500\u001b[0m outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mroberta\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1501\u001b[0m \u001b[43m    \u001b[49m\u001b[43minput_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1502\u001b[0m \u001b[43m    \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1503\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtoken_type_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtoken_type_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1504\u001b[0m \u001b[43m    \u001b[49m\u001b[43mposition_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mposition_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1505\u001b[0m \u001b[43m    \u001b[49m\u001b[43mhead_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhead_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1506\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs_embeds\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs_embeds\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1507\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1508\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_hidden_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1509\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreturn_dict\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_dict\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1510\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1512\u001b[0m sequence_output \u001b[38;5;241m=\u001b[39m outputs[\u001b[38;5;241m0\u001b[39m]\n\u001b[0;32m   1514\u001b[0m logits \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mqa_outputs(sequence_output)\n",
      "File \u001b[1;32mc:\\Python312\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Python312\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Python312\\Lib\\site-packages\\transformers\\models\\roberta\\modeling_roberta.py:835\u001b[0m, in \u001b[0;36mRobertaModel.forward\u001b[1;34m(self, input_ids, attention_mask, token_type_ids, position_ids, head_mask, inputs_embeds, encoder_hidden_states, encoder_attention_mask, past_key_values, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[0;32m    826\u001b[0m head_mask \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mget_head_mask(head_mask, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39mnum_hidden_layers)\n\u001b[0;32m    828\u001b[0m embedding_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39membeddings(\n\u001b[0;32m    829\u001b[0m     input_ids\u001b[38;5;241m=\u001b[39minput_ids,\n\u001b[0;32m    830\u001b[0m     position_ids\u001b[38;5;241m=\u001b[39mposition_ids,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    833\u001b[0m     past_key_values_length\u001b[38;5;241m=\u001b[39mpast_key_values_length,\n\u001b[0;32m    834\u001b[0m )\n\u001b[1;32m--> 835\u001b[0m encoder_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mencoder\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    836\u001b[0m \u001b[43m    \u001b[49m\u001b[43membedding_output\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    837\u001b[0m \u001b[43m    \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mextended_attention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    838\u001b[0m \u001b[43m    \u001b[49m\u001b[43mhead_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhead_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    839\u001b[0m \u001b[43m    \u001b[49m\u001b[43mencoder_hidden_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mencoder_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    840\u001b[0m \u001b[43m    \u001b[49m\u001b[43mencoder_attention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mencoder_extended_attention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    841\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpast_key_values\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpast_key_values\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    842\u001b[0m \u001b[43m    \u001b[49m\u001b[43muse_cache\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muse_cache\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    843\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    844\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_hidden_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    845\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreturn_dict\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_dict\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    846\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    847\u001b[0m sequence_output \u001b[38;5;241m=\u001b[39m encoder_outputs[\u001b[38;5;241m0\u001b[39m]\n\u001b[0;32m    848\u001b[0m pooled_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpooler(sequence_output) \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpooler \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Python312\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Python312\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Python312\\Lib\\site-packages\\transformers\\models\\roberta\\modeling_roberta.py:524\u001b[0m, in \u001b[0;36mRobertaEncoder.forward\u001b[1;34m(self, hidden_states, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, past_key_values, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[0;32m    513\u001b[0m     layer_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_gradient_checkpointing_func(\n\u001b[0;32m    514\u001b[0m         layer_module\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__call__\u001b[39m,\n\u001b[0;32m    515\u001b[0m         hidden_states,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    521\u001b[0m         output_attentions,\n\u001b[0;32m    522\u001b[0m     )\n\u001b[0;32m    523\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 524\u001b[0m     layer_outputs \u001b[38;5;241m=\u001b[39m \u001b[43mlayer_module\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    525\u001b[0m \u001b[43m        \u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    526\u001b[0m \u001b[43m        \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    527\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlayer_head_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    528\u001b[0m \u001b[43m        \u001b[49m\u001b[43mencoder_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    529\u001b[0m \u001b[43m        \u001b[49m\u001b[43mencoder_attention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    530\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpast_key_value\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    531\u001b[0m \u001b[43m        \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    532\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    534\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m layer_outputs[\u001b[38;5;241m0\u001b[39m]\n\u001b[0;32m    535\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m use_cache:\n",
      "File \u001b[1;32mc:\\Python312\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Python312\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Python312\\Lib\\site-packages\\transformers\\models\\roberta\\modeling_roberta.py:413\u001b[0m, in \u001b[0;36mRobertaLayer.forward\u001b[1;34m(self, hidden_states, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, past_key_value, output_attentions)\u001b[0m\n\u001b[0;32m    401\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\n\u001b[0;32m    402\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m    403\u001b[0m     hidden_states: torch\u001b[38;5;241m.\u001b[39mTensor,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    410\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tuple[torch\u001b[38;5;241m.\u001b[39mTensor]:\n\u001b[0;32m    411\u001b[0m     \u001b[38;5;66;03m# decoder uni-directional self-attention cached key/values tuple is at positions 1,2\u001b[39;00m\n\u001b[0;32m    412\u001b[0m     self_attn_past_key_value \u001b[38;5;241m=\u001b[39m past_key_value[:\u001b[38;5;241m2\u001b[39m] \u001b[38;5;28;01mif\u001b[39;00m past_key_value \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m--> 413\u001b[0m     self_attention_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mattention\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    414\u001b[0m \u001b[43m        \u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    415\u001b[0m \u001b[43m        \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    416\u001b[0m \u001b[43m        \u001b[49m\u001b[43mhead_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    417\u001b[0m \u001b[43m        \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    418\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpast_key_value\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mself_attn_past_key_value\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    419\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    420\u001b[0m     attention_output \u001b[38;5;241m=\u001b[39m self_attention_outputs[\u001b[38;5;241m0\u001b[39m]\n\u001b[0;32m    422\u001b[0m     \u001b[38;5;66;03m# if decoder, the last output is tuple of self-attn cache\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Python312\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Python312\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Python312\\Lib\\site-packages\\transformers\\models\\roberta\\modeling_roberta.py:340\u001b[0m, in \u001b[0;36mRobertaAttention.forward\u001b[1;34m(self, hidden_states, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, past_key_value, output_attentions)\u001b[0m\n\u001b[0;32m    330\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\n\u001b[0;32m    331\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m    332\u001b[0m     hidden_states: torch\u001b[38;5;241m.\u001b[39mTensor,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    338\u001b[0m     output_attentions: Optional[\u001b[38;5;28mbool\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[0;32m    339\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tuple[torch\u001b[38;5;241m.\u001b[39mTensor]:\n\u001b[1;32m--> 340\u001b[0m     self_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mself\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    341\u001b[0m \u001b[43m        \u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    342\u001b[0m \u001b[43m        \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    343\u001b[0m \u001b[43m        \u001b[49m\u001b[43mhead_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    344\u001b[0m \u001b[43m        \u001b[49m\u001b[43mencoder_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    345\u001b[0m \u001b[43m        \u001b[49m\u001b[43mencoder_attention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    346\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpast_key_value\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    347\u001b[0m \u001b[43m        \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    348\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    349\u001b[0m     attention_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moutput(self_outputs[\u001b[38;5;241m0\u001b[39m], hidden_states)\n\u001b[0;32m    350\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m (attention_output,) \u001b[38;5;241m+\u001b[39m self_outputs[\u001b[38;5;241m1\u001b[39m:]  \u001b[38;5;66;03m# add attentions if we output them\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Python312\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Python312\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Python312\\Lib\\site-packages\\transformers\\models\\roberta\\modeling_roberta.py:276\u001b[0m, in \u001b[0;36mRobertaSelfAttention.forward\u001b[1;34m(self, hidden_states, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, past_key_value, output_attentions)\u001b[0m\n\u001b[0;32m    273\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m head_mask \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    274\u001b[0m     attention_probs \u001b[38;5;241m=\u001b[39m attention_probs \u001b[38;5;241m*\u001b[39m head_mask\n\u001b[1;32m--> 276\u001b[0m context_layer \u001b[38;5;241m=\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmatmul\u001b[49m\u001b[43m(\u001b[49m\u001b[43mattention_probs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvalue_layer\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    278\u001b[0m context_layer \u001b[38;5;241m=\u001b[39m context_layer\u001b[38;5;241m.\u001b[39mpermute(\u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m2\u001b[39m, \u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m3\u001b[39m)\u001b[38;5;241m.\u001b[39mcontiguous()\n\u001b[0;32m    279\u001b[0m new_context_layer_shape \u001b[38;5;241m=\u001b[39m context_layer\u001b[38;5;241m.\u001b[39msize()[:\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m2\u001b[39m] \u001b[38;5;241m+\u001b[39m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mall_head_size,)\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "def retrieve_and_answer(query, model, index, sentences, qa_pipeline, top_n=5, max_chunk_length=400):\n",
    "    \"\"\"\n",
    "    Retrieves relevant sections, chunks them, scores them, and gets the answer using the QA model.\n",
    "    \"\"\"\n",
    "    # Retrieve the top N relevant sections using FAISS\n",
    "    relevant_sections = retrieve_relevant_sections(query, model, index, sentences, top_n)\n",
    "    \n",
    "    # Combine retrieved sections into one text and chunk them\n",
    "    combined_text = \" \".join(relevant_sections)\n",
    "\n",
    "    print(f\"Combined text length (in words): {len(combined_text.split())}\")\n",
    "    \n",
    "    chunks = chunk_text(combined_text, max_length=max_chunk_length)\n",
    "    \n",
    "    # Score chunks for relevance\n",
    "    chunk_scores = score_chunks(query, chunks, model)\n",
    "    \n",
    "    # Select the top chunk based on the score\n",
    "    top_chunk = chunk_scores[0][0]  # Get the chunk with the highest relevance score\n",
    "    \n",
    "    # Use the QA model to find the answer in the most relevant chunk\n",
    "    result = qa_pipeline({\n",
    "        'question': query,\n",
    "        'context': top_chunk\n",
    "    })\n",
    "    \n",
    "    return result['answer'], top_chunk\n",
    "\n",
    "# Example usage\n",
    "query = \"What are the best career options in technology?\"\n",
    "answer, top_chunk = retrieve_and_answer(query, model, index, sentences, qa_pipeline, top_n=5)\n",
    "\n",
    "print(\"Most Relevant Chunk:\", top_chunk)\n",
    "print(\"Answer:\", answer)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
