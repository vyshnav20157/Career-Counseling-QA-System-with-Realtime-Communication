{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pdfplumber\n",
    "import re\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "import csv\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\vyshn\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\vyshn\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Download NLTK data (you only need to run this once)\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a function to clean and preprocess text\n",
    "def clean_text(text):\n",
    "    \"\"\"\n",
    "    Cleans and preprocesses the text for vectorization.\n",
    "    - Removes headers, footers, special characters, and unnecessary whitespace.\n",
    "    - Tokenizes text into sentences.\n",
    "    - Removes stop words.\n",
    "    \"\"\"\n",
    "    # Remove headers and footers (assumption: they repeat every page)\n",
    "    text = re.sub(r'CareerSeva.Com\\'s Destiny Designers: A Comprehensive Guide to Career Counseling Entrepreneurship in India - 2023*|CareerSeva.Com -A2Z Career Guidance & Planning    Â© Sfurti Media Production, Pune.2023*', '', text)\n",
    "\n",
    "    # Remove special characters and excessive whitespace\n",
    "    text = re.sub(r'\\s+', ' ', text)  # Replace multiple spaces with a single space\n",
    "    text = re.sub(r'[^\\w\\s]', '', text)  # Remove special characters\n",
    "    \n",
    "    # Tokenize text into sentences\n",
    "    sentences = nltk.sent_tokenize(text)\n",
    "    \n",
    "    # Remove stop words\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    processed_sentences = []\n",
    "    for sentence in sentences:\n",
    "        words = nltk.word_tokenize(sentence)\n",
    "        filtered_words = [word for word in words if word.lower() not in stop_words]\n",
    "        processed_sentences.append(' '.join(filtered_words))\n",
    "    \n",
    "    # Return cleaned sentences as a single string\n",
    "    return ' '.join(processed_sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a function to extract text from a PDF\n",
    "def extract_text_from_pdf(pdf_path):\n",
    "    \"\"\"\n",
    "    Extracts text from a PDF file while handling images, tables, bullet points, headers, and footers.\n",
    "    \"\"\"\n",
    "    extracted_text = \"\"\n",
    "    \n",
    "    # Open the PDF file\n",
    "    with pdfplumber.open(pdf_path) as pdf:\n",
    "        # Iterate through each page\n",
    "        for page in pdf.pages:\n",
    "            # Extract text from the page\n",
    "            page_text = page.extract_text()\n",
    "            \n",
    "            # Skip pages with no text\n",
    "            if not page_text:\n",
    "                continue\n",
    "            \n",
    "            # Clean the extracted text\n",
    "            cleaned_text = clean_text(page_text)\n",
    "            \n",
    "            # Append cleaned text\n",
    "            extracted_text += cleaned_text + \" \"\n",
    "    \n",
    "    return extracted_text.strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_footer(text):\n",
    "    pattern = r\"CareerSevaCom A2Z Career Guidance Planning Sfurti Media Production Pune2023 Page \\d+\"\n",
    "    cleaned_text = re.sub(pattern, '', text)\n",
    "    return cleaned_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CareerSevaComs EBook Edition Destiny Designers 2023 Comprehensive Guide Career Counseling Entrepreneurship India Teacher Working SelfEmployed Professional Digital Content Creator Housewife Simply Graduate Want Make Rewarding Career Career Counseling Begin Journey  Unlocking Secrets Successful Career Counseling Future Educators Coaches Author Editor CareerSevacoms Expert Career Counselors Content Creation Team EBook MRP INR 299 Published Sfurti Media Production Pune  Preface fluid tapestry life dreams interweave destiny aspirations intersect reality stands beacon guidance wisdom career counselor stand precipice 2023 era shaped technological leaps environmental challenges socio economic transformations role career counselor never critical task intricate Comprehensive Training Guide Setting Successful Career Counseling Practice 2023 compilation theories strategies tools heartfelt ode every individual chosen noble pursuit lighting path others pages find latest trends insights professional \n"
     ]
    }
   ],
   "source": [
    "# Path to your PDF file\n",
    "pdf_path = 'Dataset.pdf'\n",
    "\n",
    "# Extract and preprocess text from the PDF\n",
    "processed_text = extract_text_from_pdf(pdf_path)\n",
    "\n",
    "# Remove Footer\n",
    "processed_text = remove_footer(processed_text)\n",
    "\n",
    "# Print a preview of the processed text\n",
    "print(processed_text[:1000])  # Display the first 1000 characters to verify"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_tokens_to_csv(tokens_string, filename):\n",
    "    tokens = tokens_string.split()\n",
    "    with open(filename, mode='w', newline='') as file:\n",
    "        writer = csv.writer(file)\n",
    "        writer.writerow(tokens)\n",
    "\n",
    "filename = \"tokens.csv\"\n",
    "save_tokens_to_csv(processed_text, filename)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
