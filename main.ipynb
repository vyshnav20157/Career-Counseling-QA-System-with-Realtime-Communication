{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\vyshn\\AppData\\Roaming\\Python\\Python312\\site-packages\\sentence_transformers\\cross_encoder\\CrossEncoder.py:11: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from tqdm.autonotebook import tqdm, trange\n"
     ]
    }
   ],
   "source": [
    "import pdfplumber\n",
    "import re\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "import csv\n",
    "\n",
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "import faiss\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\vyshn\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\vyshn\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Download NLTK data (you only need to run this once)\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a function to clean and preprocess text\n",
    "def clean_text(text):\n",
    "    \"\"\"\n",
    "    Cleans and preprocesses the text for vectorization.\n",
    "    - Removes headers, footers, special characters, and unnecessary whitespace.\n",
    "    - Tokenizes text into sentences.\n",
    "    - Removes stop words.\n",
    "    \"\"\"\n",
    "    # Remove headers\n",
    "    text = re.sub(r'CareerSeva.Com\\'s Destiny Designers: A Comprehensive Guide to Career Counseling Entrepreneurship in India - 2023*|CareerSeva.Com -A2Z Career Guidance & Planning    © Sfurti Media Production, Pune.2023*', '', text)\n",
    "\n",
    "    # Remove excessive whitespace\n",
    "    text = re.sub(r'\\s+', ' ', text)  # Replace multiple spaces with a single space\n",
    "    \n",
    "    # Tokenize text into sentences\n",
    "    sentences = nltk.sent_tokenize(text)\n",
    "    \n",
    "    # Remove stop words\n",
    "    # stop_words = set(stopwords.words('english'))\n",
    "    processed_sentences = []\n",
    "    for sentence in sentences:\n",
    "        words = nltk.word_tokenize(sentence)\n",
    "        filtered_words = [word for word in words]\n",
    "        processed_sentences.append(' '.join(filtered_words))\n",
    "    \n",
    "    # Return cleaned sentences as a single string\n",
    "    return ' '.join(processed_sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a function to extract text from a PDF\n",
    "def extract_text_from_pdf(pdf_path):\n",
    "    \"\"\"\n",
    "    Extracts text from a PDF file while handling images, tables, bullet points, headers, and footers.\n",
    "    \"\"\"\n",
    "    extracted_text = \"\"\n",
    "    \n",
    "    # Open the PDF file\n",
    "    with pdfplumber.open(pdf_path) as pdf:\n",
    "        # Iterate through each page\n",
    "        for page in pdf.pages:\n",
    "            # Extract text from the page\n",
    "            page_text = page.extract_text()\n",
    "            \n",
    "            # Skip pages with no text\n",
    "            if not page_text:\n",
    "                continue\n",
    "            \n",
    "            # Clean the extracted text\n",
    "            cleaned_text = clean_text(page_text)\n",
    "            \n",
    "            # Append cleaned text\n",
    "            extracted_text += cleaned_text + \" \"\n",
    "    \n",
    "    return extracted_text.strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_footer(text):\n",
    "    pattern = r\"CareerSeva.Com -A2Z Career Guidance & Planning © Sfurti Media Production , Pune.2023 Page \\d+\"\n",
    "    cleaned_text = re.sub(pattern, '', text)\n",
    "    return cleaned_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def remove_numeric_tokens(tokens_string):\n",
    "#     tokens = tokens_string.split()\n",
    "#     filtered_tokens = [token for token in tokens if not token.isdigit()]\n",
    "#     result_string = ' '.join(filtered_tokens)\n",
    "#     return result_string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def remove_tokens_between(tokens_string):\n",
    "#     \"\"\"\n",
    "#     Remove page containing table of contents\n",
    "#     \"\"\"\n",
    "#     # Split the space-separated string into a list of tokens\n",
    "#     tokens = tokens_string.split()\n",
    "    \n",
    "#     # Initialize flags for tracking token range\n",
    "#     within_range = False\n",
    "#     filtered_tokens = []\n",
    "#     count = 0\n",
    "    \n",
    "#     for token in tokens:\n",
    "#         if token == \"-:Table\":\n",
    "#             within_range = True\n",
    "#             count += 1\n",
    "        \n",
    "#         if not within_range:\n",
    "#             filtered_tokens.append(token)\n",
    "        \n",
    "#         if token == \"153\":\n",
    "#             within_range = False\n",
    "#             count -= 1\n",
    "    \n",
    "#     # Join the filtered tokens back into a space-separated string\n",
    "#     cleaned_tokens_string = ' '.join(filtered_tokens)\n",
    "    \n",
    "#     #print(count)\n",
    "#     return cleaned_tokens_string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CareerSeva.Com 's E-Book Edition Destiny Designers 2023 A Comprehensive Guide to Career Counseling Entrepreneurship in India If You Are A Teacher , Any Working or Self-Employed Professional , Digital Content Creator , Housewife Or Simply Any Graduate & Want To Make A Rewarding Career In Career Counseling ? Begin Your Journey Here ....  Unlocking The Secrets Of Successful Career Counseling For Future Educators And Coaches Author / Editor CareerSeva.com 's Expert Career Counselors & Content Creation Team E-Book MRP : INR 299/- Published By : Sfurti Media Production , Pune  Preface In the fluid tapestry of life , where dreams interweave with destiny , and aspirations intersect with reality , stands a beacon of guidance and wisdom : the career counselor . As we stand at the precipice of 2023 , an era shaped by technological leaps , environmental challenges , and socio- economic transformations , the role of the career counselor has never been more critical , nor their task more intricate .\n"
     ]
    }
   ],
   "source": [
    "# Path to your PDF file\n",
    "pdf_path = 'Dataset.pdf'\n",
    "\n",
    "# Extract and preprocess text from the PDF\n",
    "processed_text = extract_text_from_pdf(pdf_path)\n",
    "\n",
    "# Remove Footer\n",
    "processed_text = remove_footer(processed_text)\n",
    "\n",
    "# # Remove Table of Contents page\n",
    "# processed_text = remove_tokens_between(processed_text)\n",
    "\n",
    "# # Remove number tokens\n",
    "# processed_text = remove_numeric_tokens(processed_text)\n",
    "\n",
    "# Print a preview of the processed text\n",
    "print(processed_text[:1000])  # Display the first 500 characters to verify"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_tokens_to_csv(tokens_string, filename):\n",
    "    tokens = tokens_string.split()\n",
    "    with open(filename, mode='w', newline='') as file:\n",
    "        writer = csv.writer(file)\n",
    "        for token in tokens:\n",
    "            writer.writerow([token])  # Write each token on a new line\n",
    "\n",
    "filename = \"tokens.csv\"\n",
    "save_tokens_to_csv(processed_text, filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FAISS index saved with 53606 tokens.\n"
     ]
    }
   ],
   "source": [
    "# Load tokens from CSV file\n",
    "def load_tokens_from_csv(filename):\n",
    "    tokens = []\n",
    "    with open(filename, mode='r', newline='') as file:\n",
    "        reader = csv.reader(file)\n",
    "        for row in reader:\n",
    "            if row:  # Ensure row is not empty\n",
    "                tokens.append(row[0])  # Each token is stored in its own row\n",
    "    return tokens\n",
    "\n",
    "# Path to the CSV file containing tokens\n",
    "filename = \"tokens.csv\"\n",
    "\n",
    "# Load tokens\n",
    "tokens = load_tokens_from_csv(filename)\n",
    "\n",
    "# Initialize the multi-qa-mpnet-base-dot-v1 model\n",
    "model = SentenceTransformer('multi-qa-mpnet-base-dot-v1')\n",
    "\n",
    "# Encode each token into an embedding\n",
    "embeddings = model.encode(tokens)\n",
    "\n",
    "# Convert the embeddings to a numpy array\n",
    "embeddings = np.array(embeddings)\n",
    "\n",
    "# Step 2: Save the Embeddings in a FAISS Database\n",
    "\n",
    "# Get the dimension of the embeddings\n",
    "dimension = embeddings.shape[1]\n",
    "\n",
    "# Initialize a FAISS index with L2 (Euclidean) distance\n",
    "index = faiss.IndexFlatL2(dimension)\n",
    "\n",
    "# Add embeddings to the FAISS index\n",
    "index.add(embeddings)\n",
    "\n",
    "# Save the FAISS index to disk\n",
    "faiss.write_index(index, 'tokens_faiss_index.index')\n",
    "\n",
    "print(f\"FAISS index saved with {index.ntotal} tokens.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question: What is career counseling?\n",
      "Answer: a journey of self-discovery and understanding the market demand\n"
     ]
    }
   ],
   "source": [
    "import faiss\n",
    "import numpy as np\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from transformers import pipeline\n",
    "import nltk\n",
    "import csv\n",
    "\n",
    "# Load the FAISS index\n",
    "index = faiss.read_index('tokens_faiss_index.index')\n",
    "\n",
    "# Load the tokens from CSV again for reference\n",
    "def load_tokens_from_csv(filename):\n",
    "    tokens = []\n",
    "    with open(filename, mode='r', newline='') as file:\n",
    "        reader = csv.reader(file)\n",
    "        for row in reader:\n",
    "            if row:\n",
    "                tokens.append(row[0])\n",
    "    return tokens\n",
    "\n",
    "tokens = load_tokens_from_csv(\"tokens.csv\")\n",
    "\n",
    "# Load the Sentence Transformer model\n",
    "embedding_model = SentenceTransformer('multi-qa-mpnet-base-dot-v1')\n",
    "\n",
    "# Load the extractive QA model (using a model fine-tuned on SQuAD 2.0)\n",
    "qa_model = pipeline('question-answering', model='deepset/roberta-base-squad2')\n",
    "\n",
    "# Function to split the text into sentences\n",
    "def split_into_sentences(text):\n",
    "    sentences = nltk.sent_tokenize(text)\n",
    "    return sentences\n",
    "\n",
    "# Load the full text (concatenated tokens)\n",
    "full_text = ' '.join(tokens)\n",
    "\n",
    "# Split the full text into sentences\n",
    "sentences = split_into_sentences(full_text)\n",
    "\n",
    "# Create embeddings for each sentence\n",
    "sentence_embeddings = embedding_model.encode(sentences)\n",
    "\n",
    "# Initialize a new FAISS index for sentences\n",
    "dimension = sentence_embeddings.shape[1]\n",
    "sentence_index = faiss.IndexFlatL2(dimension)\n",
    "sentence_index.add(np.array(sentence_embeddings))\n",
    "\n",
    "def get_relevant_context(question, k=5):\n",
    "    \"\"\"\n",
    "    Retrieve the most relevant context (sentences) for a given question using FAISS.\n",
    "    :param question: The user's question as a string.\n",
    "    :param k: Number of most similar sentences to retrieve.\n",
    "    :return: A combined string of the most relevant sentences.\n",
    "    \"\"\"\n",
    "    # Encode the question using the same embedding model\n",
    "    question_embedding = embedding_model.encode([question])\n",
    "\n",
    "    # Search for the k most similar sentences in the FAISS index\n",
    "    distances, indices = sentence_index.search(np.array(question_embedding).astype('float32'), k)\n",
    "    \n",
    "    # Retrieve the most relevant sentences\n",
    "    relevant_sentences = [sentences[idx] for idx in indices[0]]\n",
    "    \n",
    "    # Combine the relevant sentences into a single context for QA\n",
    "    combined_context = ' '.join(relevant_sentences)\n",
    "    \n",
    "    return combined_context\n",
    "\n",
    "def answer_question(question):\n",
    "    \"\"\"\n",
    "    Answer a question using extractive QA with context retrieved from FAISS.\n",
    "    :param question: The user's question as a string.\n",
    "    :return: The answer to the question.\n",
    "    \"\"\"\n",
    "    # Get the most relevant context from the FAISS index\n",
    "    context = get_relevant_context(question)\n",
    "    \n",
    "    # Use the extractive QA model to find the answer within the retrieved context\n",
    "    answer = qa_model(question=question, context=context)\n",
    "    \n",
    "    return answer['answer']\n",
    "\n",
    "# Example usage:\n",
    "question = \"What is career counseling?\"\n",
    "answer = answer_question(question)\n",
    "print(f\"Question: {question}\\nAnswer: {answer}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nquery = \"What are the best career options in technology?\"\\nrelevant_sections = retrieve_relevant_sections(query, model, index, sentences, top_n=3)\\nprint(\"Top relevant sections:\", relevant_sections)'"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\vyshn\\AppData\\Roaming\\Python\\Python312\\site-packages\\tf_keras\\src\\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.\n",
      "\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Combined text length (in words): 35\n",
      "Generated 1 chunks:\n",
      "Chunk 1 (Length: 35 words): Career Options Suggested : 1 . Suggested Career Paths : 1 . Career Options Presented : 1 . Invest in Continuous Learning : The career landscape changes rapidly . Consider post-graduate studies for spe...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 10.99it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Most Relevant Chunk: Career Options Suggested : 1 . Suggested Career Paths : 1 . Career Options Presented : 1 . Invest in Continuous Learning : The career landscape changes rapidly . Consider post-graduate studies for specialization .\n",
      "Answer: Career Options Suggested : 1 . Suggested Career Paths\n"
     ]
    }
   ],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
